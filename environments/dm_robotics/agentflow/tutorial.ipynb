{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bs9PR2FXQl2"
      },
      "source": [
        "# \u003cdiv align=\"left\"\u003e**`agentflow` tutorial**\u003c/div\u003e\n",
        "# \u003cdiv align=\"left\"\u003e[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepmind/dm_robotics/blob/main/py/agentflow/tutorial.ipynb)\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCTYqlKaWwkx"
      },
      "source": [
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eCopyright 2021 The dm_robotics Authors.\u003c/small\u003e\u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at \u003ca href=\"http://www.apache.org/licenses/LICENSE-2.0\"\u003ehttp://www.apache.org/licenses/LICENSE-2.0\u003c/a\u003e.\u003c/small\u003e\u003c/small\u003e\u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\u003c/small\u003e\u003c/small\u003e\u003c/p\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1cOhbApnaN7"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "To run this notebook you will have to install agentflow (possibly in a [virtualenv](https://docs.python.org/3/library/venv.html)), and then build and run a Jupyter notebook.\n",
        "\n",
        "See [here](https://www.dataquest.io/blog/jupyter-notebook-tutorial/) for a tutorial on how to work with Jupyter notebooks, and [here](https://janakiev.com/blog/jupyter-virtual-envs/) for using virtualenv with Jupyter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fsyNvJcW72D"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2C4HHLsX0Kw"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "from typing import Mapping, Optional\n",
        "\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "from dm_robotics.agentflow import action_spaces\n",
        "from dm_robotics.agentflow import core\n",
        "from dm_robotics.agentflow import subtask\n",
        "from dm_robotics.agentflow.loggers import print_logger\n",
        "from dm_robotics.agentflow.loggers import subtask_logger\n",
        "from dm_robotics.agentflow.meta_options.control_flow import cond\n",
        "from dm_robotics.agentflow.meta_options.control_flow import loop_ops\n",
        "from dm_robotics.agentflow.meta_options.control_flow import sequence\n",
        "from dm_robotics.agentflow.meta_options.control_flow.examples import common\n",
        "from dm_robotics.agentflow.rendering import graphviz_renderer\n",
        "from dm_robotics.agentflow.rendering import intermediate\n",
        "from dm_robotics.agentflow.rendering import subgraph\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKdnInZ_bl3G"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi7c77UacCCB"
      },
      "source": [
        "`AgentFlow` is the task-specification framework for DeepMind Robotics. Fundamentally, it's a library for composing Reinforcement-Learning agents. The core features that AgentFlow provides are:\n",
        "\n",
        "1.  tools for slicing, transforming, and composing *specs*.\n",
        "2.  tools for encapsulating and composing *RL-tasks*.\n",
        "\n",
        "Unlike the standard RL setup, which assumes a single environment and an agent,\n",
        "`AgentFlow` is designed for the single-embodiment, multiple-task regime. This\n",
        "was motivated by the robotics use-case, which frequently requires training RL\n",
        "modules for various skills, and then composing them (possibly with non-learned\n",
        "controllers too).\n",
        "\n",
        "Instead of having to implement a separate RL environment for each skill and\n",
        "combine them ad hoc, with `AgentFlow` you can define one or more `SubTasks`\n",
        "which *modify* a timestep from a single top-level environment, e.g. adding\n",
        "observations and defining rewards, or isolating a particular sub-system of the\n",
        "environment, such as a robot arm.\n",
        "\n",
        "You then *compose* SubTasks with regular RL-agents to form modules, and use a\n",
        "set of graph-building operators to define the flow of these modules over time\n",
        "(hence the name `AgentFlow`).\n",
        "\n",
        "The graph-building step is entirely optional, and is intended only for use-cases\n",
        "that require something like a (possibly learnable, possibly stochastic)\n",
        "state-machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMpRMSpTHVq1"
      },
      "source": [
        "# Core Concepts\n",
        "This colab introduces the core concepts in AgentFlow, and illustrates the two common ways to use AgentFlow to build a robotics experiment loop.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN0e4RQzHpQV"
      },
      "source": [
        "## Policies and Options\n",
        "\n",
        "A Policy is the base-class for all agents in AgentFlow. Its primary method is:\n",
        "\n",
        "```python\n",
        "def step(self, timestep: dm_env.TimeStep) -\u003e np.ndarray:\n",
        "  ...\n",
        "```\n",
        "\n",
        "All other methods on `Policy` are for book-keeping and visualization.\n",
        "\n",
        "*** \n",
        "\n",
        "An `Option` is a `Policy` which can also:\n",
        "\n",
        "*   Decide if it's eligible to stop (expressed via a termination-probability `pterm`)\n",
        "*   Consume a runtime-argument (E.g. a pose to reach towards)\n",
        "*   Produce a result (`OptionResult`)\n",
        "\n",
        "These enable it to be used within an AgentFlow graph by other `MetaOptions`,\n",
        "which all know about the basic life-cycle semantics of options. E.g. `af.While` will repeat a given option until the condition is `False`, or the option's `pterm` samples to `True`.  Composition of multiple agentflow Options will be discussed below.\n",
        "\n",
        "The name \"Option\" is inspired from the seminal work Sutton, Precup, \u0026 Singh on temporal abstraction in Reinforcement Learning:\n",
        "\n",
        "`Sutton, Richard S., Doina Precup, and Satinder Singh. \"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning.\" Artificial intelligence 112.1-2 (1999): 181-211.`\n",
        "\n",
        "AgentFlow's `Option` is compatible with this notion of option*, but agentflow does not implement any learning algorithms. Instead it should be thought of as an API abstraction that allows expressing various algorithms involving options, along with tools for composing them to solve larger tasks.\n",
        "\n",
        "*Modulo the omission of a `pinit` method defining the initiation-set, which may be added in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drgBgVeDH-R9"
      },
      "source": [
        "## SubTask\n",
        "\n",
        "`SubTask` is the AgentFlow mechanism for defing an RL problem *in-situ*,\n",
        "i.e. in the context of a larger problem on the same underlying embodiment. \n",
        "\n",
        "The `SubTask` defines everything that an `Option` needs to run, except the policy itself. This decoupling of option-methods from policy-methods allows a subtask to train an arbitrary RL agent, which doesn't have to know it's actually living\n",
        "in an AgentFlow graph. From the agent's perspective, it just wakes up in a particular state and runs an episode until seeing a LAST timestep signifying the end of episode.\n",
        "\n",
        "**Specs.** Like a regular RL-task, `SubTask` must  define observation and action specs that define the task for the agent that it runs against. There is also an `arg_spec` mechanism on `SubTask` which defines a protocol for passing signals to children via the observation, but this is outside the scope of this colab (details can be found [here](https://github.com/deepmind/dm_robotics/blob/main/py/agentflow/docs/components.md)).\n",
        "\n",
        "**Timestep transformation.** Rather than having to generate observations from\n",
        "scratch, a `SubTask` receives the timestep from the parent (or base\n",
        "environment), and modifies it via `parent_to_agent_timestep` to suit the needs\n",
        "of the child `Policy` it is training.\n",
        "\n",
        "After stepping the policy with this timestep, it post-processes the policy's\n",
        "action with `agent_to_parent_action` to comply with the action_spec of the\n",
        "context in which the `SubTask` is defined.\n",
        "\n",
        "A SubTask can be learned or engineered, or a mixture of both. E.g. in the\n",
        "[DPGfD](https://sites.google.com/corp/view/dpgfd-insertion/home) paper we used a\n",
        "SubTask that had a learned CNN-based reward classifier, injected bottle-neck\n",
        "activations from this model for visual features (both via\n",
        "`TimestepPreprocessors`), and a hand-defined cartesian-velocity action space.\n",
        "\n",
        "**Composition.**\n",
        "A `SubTask` and `Policy` are then composed via `SubTaskOption` into an option that can be used like a regular `Option` in an AgentFlow graph (see below).\n",
        "\n",
        "**A note on logging.** An agentflow agent typically includes one more `SubTaskOption`s, each of which define an RL problem. This top-level agent is run against a [\"base\" environment](https://github.com/deepmind/dm_robotics/blob/main/py/moma/base_task.py) which exposes raw sensor observations but no rewards or termination. \n",
        "\n",
        "There are important implications of this design choice in how data are logged. Agentflow defines data-logging at the *subtask level* via `SubTaskLogger`, which logs the *raw sensor data* that the subtask sees, i.e. `parent_timestep`, along with the action returned by the `Policy`.\n",
        "This allows researchers to iterate on the subtask and agent, e.g. changing the reward function, features, etc, all without changing the underlying dataset.  This has proven to be a powerful feature, in particular for offline-RL workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP3UdK11Oo2Z"
      },
      "source": [
        "## SubTaskOption\n",
        "\n",
        "`SubTaskOption` is a container that combines a `SubTask` and a `Policy` into a full-fledged `Option` that can be run in an AgentFlow graph.\n",
        "\n",
        "Given a subtask and a policy, a SubTaskOption can be built and used as follows:\n",
        "```python\n",
        "learned_skill = SubTaskOption(subtask, agent)\n",
        "Sequence([scripted_action, learned_skill, ...])\n",
        "```\n",
        "\n",
        "In this example we used `Sequence` to define a linear sequence of options. Control-flow over options will be discussed in greater detail below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWO2XZFqIBkj"
      },
      "source": [
        "## Action Spaces\n",
        "\n",
        "A `SubTask` allows the action space for the policy it interacts with to be different than the action space of the parent task. This is achieved by mapping the action an agent returns to it to an action that the larger task (environment) can consume.\n",
        "\n",
        "In AgentFlow the role of action-mapping in a SubTask is handled by an `ActionSpace`. This class has a `project` method which works as follows:\n",
        "\n",
        "`parent_action = space.project(child_action)`.\n",
        "\n",
        "AgentFlow comes with a small but powerful [action_space library](https://github.com/deepmind/dm_robotics/blob/main/py/agentflow/action_spaces.py) that's designed to work with action_specs that contain tab-delimited `name` field listing all the actuated-DOF (as we get from [Composer](https://github.com/deepmind/dm_control/tree/master/dm_control/composer)).  These actions can be sliced and recombined using `prefix_slicer` and `CompositeActionSpace`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZFS7p0AK1ep"
      },
      "outputs": [],
      "source": [
        "full_spec = specs.BoundedArray(\n",
        "    shape=(7,),\n",
        "    minimum=[-1] * 7,\n",
        "    maximum=[1] * 7,\n",
        "    dtype=np.float32,\n",
        "    name='arm/l0\\tarm/l1\\tarm/l2\\tarm/l3\\tarm/l4\\tarm/l5\\tgripper')\n",
        "\n",
        "# Define a sub-space for the arm.\n",
        "arm_space = action_spaces.prefix_slicer(full_spec, prefix='arm')\n",
        "\n",
        "# The spec for this space is just 6DOF.\n",
        "print(f'Action spec for just the arm:\\n\\t{arm_space.spec()}')\n",
        "\n",
        "# We can take a 6D action satisfying this spec, and project it to the original\n",
        "# space. Note the NaN where the gripper action is missing.\n",
        "arm_action = np.ones(6) * 0.2\n",
        "arm_action_in_full_space = arm_space.project(arm_action)\n",
        "print(f'Arm action in full space: {arm_action_in_full_space}')\n",
        "\n",
        "# Now let's define another sub-space for the gripper.\n",
        "gripper_space = action_spaces.prefix_slicer(full_spec, prefix='gripper')\n",
        "# Note the gripper's action_spec is just 1-DOF\n",
        "print(f'Gripper action shape: {gripper_space.spec().shape}')\n",
        "\n",
        "# We can now project the gripper action in the full space. Note that it is\n",
        "# properly aligned at the end of the array, padded by NaNs for the arm dims.\n",
        "gripper_action = np.ones(1) * 0.5\n",
        "gripper_action_in_full_space = gripper_space.project(gripper_action)\n",
        "print(f'Gripper action in full space: {gripper_action_in_full_space}')\n",
        "\n",
        "# How do we put these together? That's where `CompositeActionSpace` comes in.\n",
        "# It's an action space that composes action spaces of equal shape, using `NaN`\n",
        "# as a mask.\n",
        "composite_space = action_spaces.CompositeActionSpace(\n",
        "    [arm_space, gripper_space],  # Order of spaces determines order of actions.\n",
        "    'full_action_space')\n",
        "full_action = composite_space.project(\n",
        "    np.concatenate((arm_action, gripper_action)).astype(np.float32))\n",
        "print(f'Full action: {full_action}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sWXOJXAHYa7"
      },
      "source": [
        "## Parameterized SubTask\n",
        "\n",
        "The `SubTask` API has a single method `parent_to_agent_timestep` for transforming an observation from the parent (or environment) into an action for the agent.  \n",
        "\n",
        "This is sufficient for defining a workflow for one or more agents, but can often lead to a proliferation of subtasks to handle small changes in the observation or reward.  AgentFlow therefore includes a special subtask called `ParameterizedSubtask`, which allows users to build subtasks out of simpler, reusable components.\n",
        "\n",
        "### Timestep Preprocessors\n",
        "\"Timestep Preprocessor\" is a mechanism for defining timestep-transformations via a chain of simpler timestep-transformations. AgentFlow comes with a library of useful preprocessors, and new ones can be added by subclassing `TimestepPreprocessor`\n",
        "\n",
        "In conjunction with `SequentialActionSpace` (which defines the same concept for actions), we can build a rich space of subtasks without ever subclassing `agentflow.ParameterizedSubTask`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ddYDGAeHtOh"
      },
      "source": [
        "## Control-Flow\n",
        "\n",
        "One of the main design requirements of AgentFlow is the ability to define multiple `Options`, all of which adhere to the `dm_env` API, and then compose them to produce a larger agent.  \n",
        "\n",
        "In AgentFlow this is achieved by a \"MetaOption\", which is simply an option that can control the execution of one or more children.\n",
        "\n",
        "AgentFlow ships with a small but expressive set of control-flow \"operators\" including `Cond`, `Sequence`, `While`, and `Repeat`, which can be used to build [behavior-tree](https://en.wikipedia.org/wiki/Behavior_tree_(artificial_intelligence,_robotics_and_control))-like agents. It is also possible to define more general state-machine `MetaOption`s, but these are not included in the core library.\n",
        "\n",
        "### Example\n",
        "Let's say we're modeling an insertion problem, and we have a learned `SubTaskOption` which we only want to run if we're close to the socket.  We can define a `Cond` which branches on this condition, and calls the agent only if the predicate `near_socket` is true:\n",
        "```python\n",
        "reach_or_insert_op = af.Cond(\n",
        "    cond=near_socket,\n",
        "    true_branch=learned_insert_option,\n",
        "    false_branch=reach_option,\n",
        "    name='Reach or Insert')\n",
        "```\n",
        "This option will terminate when whichever branch is selected terminates.\n",
        "\n",
        "Let's say we then want to try this option 5 times.  We can achieve this by using a `Repeat` option:\n",
        "```python\n",
        "reach_and_insert_5x = af.Repeat(\n",
        "    5, reach_or_insert_op, name='Retry Loop')\n",
        "```\n",
        "\n",
        "Finally, we can compose this with a scripted reset policy and define our overall run-loop:\n",
        "```python\n",
        "loop_body = af.Sequence([\n",
        "    scripted_reset,\n",
        "    reach_and_insert_5x,\n",
        "    af.Cond(\n",
        "        cond=last_option_successful,\n",
        "        true_branch=extract_option,\n",
        "        false_branch=recovery_option,\n",
        "        name='post-insert')\n",
        "])\n",
        "main_loop = af.While(lambda _: True, loop_body)\n",
        "```\n",
        "\n",
        "This option can be run against a \"base\" RL environment, which typically provides raw sensor observations but doesn't otherwise define rewards or reset, which is instead handled by our `SubTask`. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S68gQz7ym7cQ"
      },
      "source": [
        "## Graph Rendering\n",
        "\n",
        "AgentFlow ships with a small graphviz-based rendering library for visualizing agent-graphs.\n",
        "\n",
        "The cell below visualizes the control-flow example above.  For this example to work pydot must be installed on the notebook machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tqOdaP3hQHQ"
      },
      "outputs": [],
      "source": [
        "# @title {form-width: \"25%\"}\n",
        "\n",
        "def near_socket(unused_timestep: dm_env.TimeStep,\n",
        "                unused_result: Optional[core.OptionResult]) -\u003e bool:\n",
        "  return False\n",
        "\n",
        "\n",
        "def last_option_successful(unused_timestep: dm_env.TimeStep,\n",
        "                           result: core.OptionResult):\n",
        "  return result.termination_reason == core.TerminationType.SUCCESS\n",
        "\n",
        "env = common.DummyEnv()\n",
        "\n",
        "# Define a subtask that exposes the desired RL-environment view on `base_task`\n",
        "my_subtask = common.DummySubTask(env.observation_spec(), 'Insertion SubTask')\n",
        "\n",
        "# Define a regular RL agent against this task-spec.\n",
        "my_policy = common.DummyPolicy(my_subtask.action_spec(),\n",
        "                                my_subtask.observation_spec(), 'My Policy')\n",
        "\n",
        "# Compose the policy and subtask to form an Option module.\n",
        "learned_insert_option = subtask.SubTaskOption(\n",
        "    my_subtask, my_policy, name='Learned Insertion')\n",
        "\n",
        "reach_option = common.DummyOption(env.action_spec(), env.observation_spec(),\n",
        "                                  'Reach for Socket')\n",
        "scripted_reset = common.DummyOption(env.action_spec(), env.observation_spec(),\n",
        "                                    'Scripted Reset')\n",
        "extract_option = common.DummyOption(env.action_spec(), env.observation_spec(),\n",
        "                                    'Extract')\n",
        "recovery_option = common.DummyOption(env.action_spec(),\n",
        "                                      env.observation_spec(), 'Recover')\n",
        "\n",
        "# Use some AgentFlow operators to embed the agent in a bigger agent.\n",
        "# First use Cond to op run learned-agent if sufficiently close.\n",
        "reach_or_insert_op = cond.Cond(\n",
        "    cond=near_socket,\n",
        "    true_branch=learned_insert_option,\n",
        "    false_branch=reach_option,\n",
        "    name='Reach or Insert')\n",
        "\n",
        "# Loop the insert-or-reach option 5 times.\n",
        "reach_and_insert_5x = loop_ops.Repeat(\n",
        "    5, reach_or_insert_op, name='Retry Loop')\n",
        "\n",
        "loop_body = sequence.Sequence([\n",
        "    scripted_reset,\n",
        "    reach_and_insert_5x,\n",
        "    cond.Cond(\n",
        "        cond=last_option_successful,\n",
        "        true_branch=extract_option,\n",
        "        false_branch=recovery_option,\n",
        "        name='post-insert')\n",
        "])\n",
        "main_loop = loop_ops.While(lambda _: True, loop_body)\n",
        "\n",
        "# Render the agentflow graph\n",
        "render_graph = False  # @param {type: \"boolean\"}\n",
        "if render_graph:\n",
        "  # `dot` must be installed on host machine for this to work.\n",
        "  graph = intermediate.render(main_loop)\n",
        "  graphviz_graph = graphviz_renderer.render(graph)\n",
        "  Image(graphviz_renderer.to_png(graphviz_graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlqwKfGcUXL"
      },
      "source": [
        "# Direct-Dispatch Example\n",
        "\n",
        "This example demonstrates a way to build scripted and learned AgentFlow components that directly wrap aribitrary python callables for communicating with an *external* system (e.g. a robot). \n",
        "\n",
        "The primary advantage of this workflow, vs. the environment-dispatch below, is simplicity and modularity.  It's entirely up to the user how to obtain observation and dispatch actions, and all\n",
        "data is local to the individual agentflow modules (Policy, Option).\n",
        "\n",
        "For the alternative \"environment-dispatch\" model see `environment_dispatch_workflow.py`\n",
        "\n",
        "High level workflow:\n",
        "1. Implement stubs for receiving observation and sending actions.\n",
        "1. Create an AgentFlow policy that generates valid actions, e.g. from a neural-net.\n",
        "1. Create a subtask that holds the agent and the I/O callbacks, and allows us to attach a logging observer to record data.\n",
        "1. Create an logging observer and attach to agent.\n",
        "1. Create an AgentFlow `Option` implementing the desired reset behaviour.\n",
        "1. Create a run loop and go.\n",
        "\n",
        "Notes:\n",
        "  * This workflow is blocking iff the observation or action stubs block.  For an\n",
        "     RPC-style interface consider dm_env_rpc (useful if env and agent live in\n",
        "    different processes) or a custom-RPC service.\n",
        "  * The `ActionCallback` currently lives in the environment, but it could easily\n",
        "    be moved closer to the agent, e.g. in the SubTask (as an ActionSpace) or the\n",
        "    Policy itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAN-RY22vWH8"
      },
      "source": [
        "## I/O Stubs\n",
        "We begin by defining callbacks which send actions and receive sensor observations. These are stubs in the example, but are where a user would typically interface with the robot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-pSFN4Gvq0O"
      },
      "outputs": [],
      "source": [
        "def observation_update_stub() -\u003e np.ndarray:\n",
        "  observation = np.random.rand(4)\n",
        "  print(f\"observation_update_stub called! Returning observation {observation}\")\n",
        "  return observation\n",
        "\n",
        "\n",
        "def send_action_stub(action: np.ndarray) -\u003e None:\n",
        "  print(f\"send_action_stub called with {action}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3q0SRlzvsDq"
      },
      "source": [
        "## Callbacks\n",
        "\n",
        "Here we wrap these callbacks with types to associate specs with the input and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVjismakoivZ"
      },
      "outputs": [],
      "source": [
        "class ObservationCallback(abc.ABC):\n",
        "  \"\"\"Base class for observation-callbacks which pull data from the world.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self) -\u003e Mapping[str, np.ndarray]:\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    pass\n",
        "\n",
        "\n",
        "class ActionCallback(abc.ABC):\n",
        "  \"\"\"Base class for action-callbacks which send actions to the world.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self, action: np.ndarray) -\u003e None:\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    pass\n",
        "\n",
        "\n",
        "class ExampleObservationUpdater(ObservationCallback):\n",
        "  \"\"\"Example Observation-Update callback.\"\"\"\n",
        "\n",
        "  def __call__(self) -\u003e Mapping[str, np.ndarray]:\n",
        "    return {\"stub_observation\": observation_update_stub()}\n",
        "\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    return {\n",
        "        \"stub_observation\":\n",
        "            specs.Array((4,), dtype=np.float64, name=\"stub_observation\")\n",
        "    }\n",
        "\n",
        "\n",
        "class ExampleActionSender(ActionCallback):\n",
        "  \"\"\"Example SendAction callback.\"\"\"\n",
        "\n",
        "  def __call__(self, action: np.ndarray) -\u003e None:\n",
        "    send_action_stub(action)\n",
        "\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    return specs.BoundedArray((2,),\n",
        "                              dtype=np.float64,\n",
        "                              minimum=-np.ones(2),\n",
        "                              maximum=np.ones(2),\n",
        "                              name=\"stub action\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcXMkEncw1T5"
      },
      "source": [
        "## Defining a Scripted Behavior\n",
        "\n",
        "Now that we have action and observation callbacks we are ready to build our first \"agent\".  This agent will be a scripted module that generates random actions, but the control logic should be whatever the task requires (e.g. a \"reset\" behavior)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYsY9Z6Prm48"
      },
      "outputs": [],
      "source": [
        "class ExampleScriptedOption(core.Option):\n",
        "  \"\"\"Stub option for running scripted controller.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               observation_cb: ObservationCallback,\n",
        "               action_cb: ActionCallback,\n",
        "               name: str,\n",
        "               max_steps: int):\n",
        "    super().__init__(name)\n",
        "    self._observation_cb = observation_cb\n",
        "    self._action_cb = action_cb\n",
        "    self._action_spec = action_cb.action_spec()\n",
        "    self._max_steps = max_steps\n",
        "    self._step_idx = 0\n",
        "\n",
        "  def step(self, timestep: dm_env.TimeStep) -\u003e np.ndarray:\n",
        "    # In the direct-dispatch case the timestep argument serves only to indicate\n",
        "    # first/mid/last step.  The observation (and rewards \u0026 discount) should be\n",
        "    # overridden by the user.\n",
        "    obs = self._observation_cb()\n",
        "    timestep = timestep._replace(observation=obs)\n",
        "\n",
        "    if timestep.first():\n",
        "      self._step_idx = 0\n",
        "\n",
        "    self._step_idx += 1\n",
        "\n",
        "    # Run controller.\n",
        "    # \u003c\u003c write scripted agent logic here \u003e\u003e\n",
        "    action = np.random.rand(2).astype(self._action_spec.dtype)\n",
        "\n",
        "    # Dispatch action.\n",
        "    self._action_cb(action)\n",
        "\n",
        "    return action  # Return value unused in direct-dispatch case.\n",
        "\n",
        "  def pterm(self, timestep: dm_env.TimeStep) -\u003e float:\n",
        "    del timestep\n",
        "    if self._step_idx \u003e= self._max_steps:\n",
        "      print(f\"Terminating option because max_steps reached {self._step_idx}.\")\n",
        "      return 1.\n",
        "    return 0.\n",
        "\n",
        "  def result(self, unused_timestep: dm_env.TimeStep) -\u003e core.OptionResult:\n",
        "    return core.OptionResult(termination_reason=core.TerminationType.SUCCESS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxY1YKpOwsdj"
      },
      "source": [
        "## Define a SubTask\n",
        "\n",
        "We want to build a trainable policy next, but first we need to build a subtask for it.  You can think of a subtask as a modular version of `dm_env.Environment` -- like an Environment it determines the observations that the agent sees, and consumes its actions.  However, unlike an Environment, SubTasks *modify* observations from a higher-level environment or subtask, and *postprocess* actions to be returned to that environment.  By combining a subtask with a policy into a `SubTaskOption`, the policy can be used like any other agentflow Option.  In this fashion, users are free to build complex agents containing mixtures of scripted and learned policies, each of which see different parts of the observation space and control different parts of the action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpSgo64U8T7s"
      },
      "outputs": [],
      "source": [
        "class ExampleSubTask(subtask.SubTask):\n",
        "  \"\"\"A subtask that pulls state and sends actions directly via callbacks.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               observation_cb: ObservationCallback,\n",
        "               action_cb: ActionCallback,\n",
        "               max_steps: int):\n",
        "    super().__init__()\n",
        "    self._observation_cb = observation_cb\n",
        "    self._action_cb = action_cb\n",
        "    self._observation_spec = observation_cb.observation_spec()\n",
        "    self._action_spec = action_cb.action_spec()\n",
        "    self._max_steps = max_steps\n",
        "    self._step_idx = 0.\n",
        "\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    \"\"\"Defines the observation seen by agents for trained on this subtask.\"\"\"\n",
        "    return self._observation_spec\n",
        "\n",
        "  def reward_spec(self) -\u003e specs.Array:\n",
        "    return specs.Array(shape=(), dtype=np.float64, name=\"reward\")\n",
        "\n",
        "  def discount_spec(self) -\u003e specs.Array:\n",
        "    return specs.BoundedArray(\n",
        "        shape=(), dtype=np.float64, minimum=0., maximum=1., name=\"discount\")\n",
        "\n",
        "  def arg_spec(self) -\u003e Optional[specs.Array]:\n",
        "    \"\"\"Defines the arg to be passed by the parent task during each step.\"\"\"\n",
        "    return  # This example doesn't use parameterized-options.\n",
        "\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    \"\"\"Defines the action spec seen by agents that run on this subtask.\"\"\"\n",
        "    return self._action_spec\n",
        "\n",
        "  def agent_to_parent_action(self, agent_action: np.ndarray) -\u003e np.ndarray:\n",
        "    \"\"\"Receives agent action and dispatches to the action callback.\"\"\"\n",
        "    self._action_cb(agent_action)\n",
        "    return agent_action  # Return value unused in direct-dispatch case.\n",
        "\n",
        "  def parent_to_agent_timestep(self, parent_timestep: dm_env.TimeStep,\n",
        "                               own_arg_key: str) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Pulls the latest observation and packs to timestep for the agent.\"\"\"\n",
        "    if parent_timestep.first():\n",
        "      self._step_idx = 0.\n",
        "\n",
        "    obs = self._observation_cb()\n",
        "    agent_timestep = parent_timestep._replace(\n",
        "        observation=obs, reward=self._step_idx, discount=1.)\n",
        "\n",
        "    self._step_idx += 1\n",
        "    return agent_timestep\n",
        "\n",
        "  def pterm(self, parent_timestep: dm_env.TimeStep,\n",
        "            own_arg_key: str) -\u003e float:\n",
        "    if self._step_idx \u003e= self._max_steps:\n",
        "      print(f\"Terminating subtask because max_steps reached {self._step_idx}.\")\n",
        "      return 1.\n",
        "    return 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WszDVW-By8AP"
      },
      "source": [
        "## Define a Policy\n",
        "\n",
        "A `Policy` is the lowest-level agent type in AgentFlow.  It only defines a `step` function, and has no notion of \"pterm\", \"subtask\", \"option\", etc.\n",
        "A policy is typically where a user would implement a learning algorithm, e.g. an [acme](https://github.com/deepmind/acme) Actor that logs timesteps to replay and runs a Learner to update parameters.\n",
        "\n",
        "This learner need not know its context in an agentflow graph.  From its perspective, it \"wakes up\" when its SubTaskOption is invoked, seeing a FIRST timestep, and runs until a LAST timestep is seen.\n",
        "\n",
        "It is only by combing a `Policy` with a `SubTask` that we can build `Options` which can be composed using agentflow graph operators, e.g. `Cond`, `Sequence`, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAeQlV7Kr5-k"
      },
      "outputs": [],
      "source": [
        "class ExamplePolicy(core.Policy):\n",
        "  \"\"\"Stub policy for running learning machinery.\"\"\"\n",
        "\n",
        "  def __init__(self, action_spec: specs.BoundedArray, name: str):\n",
        "    super().__init__(name)\n",
        "    self._action_spec = action_spec\n",
        "\n",
        "  def step(self, timestep: dm_env.TimeStep) -\u003e np.ndarray:\n",
        "    return np.random.rand(2).astype(self._action_spec.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1dgy9vSWWrG"
      },
      "source": [
        "## Dummy Environment\n",
        "Any RL workflow needs an environment, but in the direct-dispatch workflow it's rather superfluous. All the observations and actions are handled by callable that sit inside the agentflow graph.  This environment exists purely to satisfy the run-loop contract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyEp82HarvIO"
      },
      "outputs": [],
      "source": [
        "class DummyEnvironment(dm_env.Environment):\n",
        "  \"\"\"A dummy environment to use in a run-loop.\"\"\"\n",
        "\n",
        "  def reset(self) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Returns the first `TimeStep` of a new episode.\"\"\"\n",
        "    return dm_env.restart({})\n",
        "\n",
        "  def step(self, unused_action: np.ndarray) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Updates the environment according to the action.\"\"\"\n",
        "    return dm_env.transition(0., {})\n",
        "\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    \"\"\"Returns the observation spec.\"\"\"\n",
        "    return {}\n",
        "\n",
        "  def action_spec(self) -\u003e specs.Array:\n",
        "    \"\"\"Returns the action spec.\"\"\"\n",
        "    return specs.Array((), dtype=np.float64, name=\"dummy_action\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20D4HbSqex0h"
      },
      "source": [
        "## Main Loop\n",
        "\n",
        "We can finally build our graph and run it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l63e9IDmrqAJ"
      },
      "outputs": [],
      "source": [
        "# Stubs for pulling observation and sending action to some external system.\n",
        "observation_cb = ExampleObservationUpdater()\n",
        "action_cb = ExampleActionSender()\n",
        "\n",
        "# Create an environment that forwards the observation and action calls.\n",
        "env = DummyEnvironment()\n",
        "\n",
        "# Stub policy that runs the desired agent.\n",
        "policy = ExamplePolicy(action_cb.action_spec(), \"agent\")\n",
        "\n",
        "# Wrap policy into an agent that logs to the terminal.\n",
        "task = ExampleSubTask(observation_cb, action_cb, 10)\n",
        "logger = print_logger.PrintLogger()\n",
        "aggregator = subtask_logger.EpisodeReturnAggregator()\n",
        "logging_observer = subtask_logger.SubTaskLogger(logger, aggregator)\n",
        "agent = subtask.SubTaskOption(task, policy, [logging_observer])\n",
        "\n",
        "reset_op = ExampleScriptedOption(observation_cb, action_cb, \"reset\", 3)\n",
        "main_loop = loop_ops.Repeat(5, sequence.Sequence([reset_op, agent]))\n",
        "\n",
        "# Run the episode.\n",
        "timestep = env.reset()\n",
        "while True:\n",
        "  action = main_loop.step(timestep)\n",
        "  timestep = env.step(action)\n",
        "\n",
        "  # Terminate if the environment or main_loop requests it.\n",
        "  if timestep.last() or (main_loop.pterm(timestep) \u003e np.random.rand()):\n",
        "    if not timestep.last():\n",
        "      termination_timestep = timestep._replace(step_type=dm_env.StepType.LAST)\n",
        "      main_loop.step(termination_timestep)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkdIHOFvs_hC"
      },
      "source": [
        "# Environment-Dispatch Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uL5VSKgtOnx"
      },
      "source": [
        "This example demonstrates a way to build an AgentFlow experiment that combine a scripted and RL policy.  In this workflow, all robot I/O is handled by the `Environment`, which is backed by arbitrary python\n",
        "callables.  This workflow may be more familiar to users accustomed to dm_env[`dm_env`](https://github.com/deepmind/dm_env), and is the way [MoMa](https://github.com/deepmind/dm_robotics/tree/main/py/moma) is written.\n",
        "\n",
        "The primary advantage of this workflow is that all I/O with the robot is\n",
        "centralized in the `ProxyEnvironment`.  This can simplify logging and debugging,\n",
        "and also allow for system designs in which agents can operate on the output of\n",
        "other agents (by nesting).\n",
        "\n",
        "For the alternative \"direct-dispatch\" model see `direct_dispatch_workflow.py`\n",
        "\n",
        "High level steps:\n",
        "1. Implement stubs for receiving state and sending actions.\n",
        "2. Provide stubs to a proxy env that manages I/O.\n",
        "3. Create an AgentFlow policy that generates valid actions, e.g. from a neural-network.\n",
        "4. Create a no-op subtask to hold the agent so we can attach a logging observer.\n",
        "5. Create an logging observer and attach to agent.\n",
        "6. Create an AgentFlow `Option` implementing the desired reset behaviour.\n",
        "7. Create a run loop and go.\n",
        "\n",
        "Notes:\n",
        "  * This workflow is blocking iff the state or action stubs block.  For an RPC-\n",
        "    style interface consider dm_env_rpc (useful if env and agent live in\n",
        "    different processes) or a custom-RPC service.\n",
        "  * The `ActionCallback` currently lives in the environment, but it could easily\n",
        "    be moved closer to the agent, e.g. in the SubTask (as an ActionSpace) or the\n",
        "    Policy itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBylrw9YwVI"
      },
      "source": [
        "## Callbacks\n",
        "\n",
        "The callbacks mechanism in this example is identical to the direct-dispatch example.  They differ in how the callbacks are used below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNZupeXdtI7W"
      },
      "outputs": [],
      "source": [
        "class ObservationCallback(abc.ABC):\n",
        "  \"\"\"Base class for state-callbacks.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self) -\u003e Mapping[str, np.ndarray]:\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    pass\n",
        "\n",
        "\n",
        "class ActionCallback(abc.ABC):\n",
        "  \"\"\"Base class for action-callbacks.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self, action: np.ndarray) -\u003e None:\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    pass\n",
        "\n",
        "\n",
        "def observation_update_stub() -\u003e np.ndarray:\n",
        "  observation = np.random.rand(4)\n",
        "  print(f\"observation_update_stub called! Returning observation {observation}\")\n",
        "  return observation\n",
        "\n",
        "\n",
        "def send_action_stub(action: np.ndarray) -\u003e None:\n",
        "  print(f\"send_action_stub called with {action}!\")\n",
        "\n",
        "\n",
        "class ExampleObservationUpdater(ObservationCallback):\n",
        "  \"\"\"Example State-Update callback.\"\"\"\n",
        "\n",
        "  def __call__(self) -\u003e Mapping[str, np.ndarray]:\n",
        "    return {\"stub_observation\": observation_update_stub()}\n",
        "\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    return {\n",
        "        \"stub_observation\":\n",
        "            specs.Array((4,), dtype=np.float64, name=\"stub_observation\")\n",
        "    }\n",
        "\n",
        "\n",
        "class ExampleActionSender(ActionCallback):\n",
        "  \"\"\"Example SendAction callback.\"\"\"\n",
        "\n",
        "  def __call__(self, action: np.ndarray) -\u003e None:\n",
        "    send_action_stub(action)\n",
        "\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    return specs.BoundedArray((2,),\n",
        "                              dtype=np.float64,\n",
        "                              minimum=-np.ones(2),\n",
        "                              maximum=np.ones(2),\n",
        "                              name=\"stub action\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4dHYRmhZ3PT"
      },
      "source": [
        "## Define a Subtask\n",
        "\n",
        "A subtask's job is to generate observations for the agent and receive its actions.  Whereas the  direct-dispatch workflow achieved this by passing the I/O callbacks directly to the subtask, in the environment-dispatch model the environment will own these callbacks.  Instead, the SubTask operates as an intermediate between the environment and the agent, allowing the agent's *view* of the environment to be customized.  In a typical use-case this would involve defining various timestep-transformations, and perhaps slicing a subset of the action space.  However, in this example we simply forward the observation and action specs of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTVx-SV-upEl"
      },
      "outputs": [],
      "source": [
        "class ExampleSubTask(subtask.SubTask):\n",
        "  \"\"\"A No-op subtask that allows us to attach a logging observer.\"\"\"\n",
        "\n",
        "  def __init__(self, observation_spec: Mapping[str, specs.Array],\n",
        "               action_spec: specs.BoundedArray,\n",
        "               max_steps: int):\n",
        "    super().__init__()\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    self._max_steps = max_steps\n",
        "    self._step_idx = 0.\n",
        "\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    \"\"\"Defines the observation seen by agents for trained on this subtask.\"\"\"\n",
        "    return self._observation_spec\n",
        "\n",
        "  def reward_spec(self) -\u003e specs.Array:\n",
        "    return specs.Array(shape=(), dtype=np.float64, name=\"reward\")\n",
        "\n",
        "  def discount_spec(self) -\u003e specs.Array:\n",
        "    return specs.BoundedArray(\n",
        "        shape=(), dtype=np.float64, minimum=0., maximum=1., name=\"discount\")\n",
        "\n",
        "  def arg_spec(self) -\u003e Optional[specs.Array]:\n",
        "    \"\"\"Defines the arg to be passed by the parent task during each step.\"\"\"\n",
        "    return  # This example doesn't use parameterized-options.\n",
        "\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    \"\"\"Defines the action spec seen by agents that run on this subtask.\"\"\"\n",
        "    return self._action_spec\n",
        "\n",
        "  def agent_to_parent_action(self, agent_action: np.ndarray) -\u003e np.ndarray:\n",
        "    \"\"\"Convert an action from the agent to the parent task.\"\"\"\n",
        "    return agent_action\n",
        "\n",
        "  def parent_to_agent_timestep(self, parent_timestep: dm_env.TimeStep,\n",
        "                               own_arg_key: str) -\u003e dm_env.TimeStep:\n",
        "    if parent_timestep.first():\n",
        "      self._step_idx = 0.\n",
        "    self._step_idx += 1\n",
        "    return parent_timestep\n",
        "\n",
        "  def pterm(self, parent_timestep: dm_env.TimeStep,\n",
        "            own_arg_key: str) -\u003e float:\n",
        "    if self._step_idx \u003e= self._max_steps:\n",
        "      print(f\"Terminating subtask because max_steps reached {self._step_idx}.\")\n",
        "      return 1.\n",
        "    return 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgXURSa6b10R"
      },
      "source": [
        "## Define a Policy\n",
        "\n",
        "The policy and scripted-behavior are no different in this workflow as in the direct-dispatch workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_uIM4K6utMV"
      },
      "outputs": [],
      "source": [
        "class ExamplePolicy(core.Policy):\n",
        "  \"\"\"Stub policy for running learning machinery.\"\"\"\n",
        "\n",
        "  def __init__(self, action_spec: specs.BoundedArray, name: str):\n",
        "    super().__init__(name)\n",
        "    self._action_spec = action_spec\n",
        "\n",
        "  def step(self, timestep: dm_env.TimeStep) -\u003e np.ndarray:\n",
        "    return np.random.rand(2).astype(self._action_spec.dtype)  # Bounds...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prj3EENDuuAw"
      },
      "outputs": [],
      "source": [
        "class ExampleScriptedOption(core.Option):\n",
        "  \"\"\"Stub option for running scripted controller.\"\"\"\n",
        "\n",
        "  def __init__(self, action_spec: specs.BoundedArray,\n",
        "               name: str,\n",
        "               max_steps: int):\n",
        "    super().__init__(name)\n",
        "    self._action_spec = action_spec\n",
        "    self._max_steps = max_steps\n",
        "    self._step_idx = 0\n",
        "\n",
        "  def step(self, timestep: dm_env.TimeStep) -\u003e np.ndarray:\n",
        "    if timestep.first():\n",
        "      self._step_idx = 0\n",
        "    self._step_idx += 1\n",
        "    return np.random.rand(2).astype(self._action_spec.dtype)  # Run controller\n",
        "\n",
        "  def pterm(self, timestep: dm_env.TimeStep) -\u003e float:\n",
        "    del timestep\n",
        "    if self._step_idx \u003e= self._max_steps:\n",
        "      print(f\"Terminating option because max_steps reached {self._step_idx}.\")\n",
        "      return 1.\n",
        "    return 0.\n",
        "\n",
        "  def result(self, unused_timestep: dm_env.TimeStep) -\u003e core.OptionResult:\n",
        "    return core.OptionResult(termination_reason=core.TerminationType.SUCCESS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ykSanTofi5L"
      },
      "source": [
        "## Define Environment\n",
        "\n",
        "Unlike the direct-dispatch workflow, the environment actually has an important role here. We pass the observation and action callbacks to a `ProxyEnvironment`, which allows it to provide an observation and action spec for our agent.  \n",
        "\n",
        "`ProxyEnvironment` is nearly a complete RL environment, except that it's missing rewards, termination, and a reset behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll5S_LH_ut4S"
      },
      "outputs": [],
      "source": [
        "class ProxyEnvironment(dm_env.Environment):\n",
        "  \"\"\"An environment that receives observation from a callback.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               observation_cb: ObservationCallback,\n",
        "               action_cb: Optional[ActionCallback] = None):\n",
        "    \"\"\"Initializes ProxyEnvironment.\n",
        "\n",
        "    Args:\n",
        "      observation_cb: An object for retrieving observation from somewhere.\n",
        "      action_cb: An object for dispatching agent actions. If `None` actions are\n",
        "        discarded\n",
        "    \"\"\"\n",
        "    self._observation_cb = observation_cb\n",
        "    self._action_cb = action_cb\n",
        "\n",
        "  def reset(self) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Returns the first `TimeStep` of a new episode.\"\"\"\n",
        "    return dm_env.restart(observation=self._observation_cb())\n",
        "\n",
        "  def step(self, action: np.ndarray) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Updates the environment according to the action.\"\"\"\n",
        "    if self._action_cb:\n",
        "      self._action_cb(action)\n",
        "    return dm_env.transition(reward=0.,\n",
        "                             discount=1.,\n",
        "                             observation=self._observation_cb())\n",
        "\n",
        "  def observation_spec(self) -\u003e Mapping[str, specs.Array]:\n",
        "    \"\"\"Returns the observation spec.\"\"\"\n",
        "    return self._observation_cb.observation_spec()\n",
        "\n",
        "  def action_spec(self) -\u003e specs.BoundedArray:\n",
        "    \"\"\"Returns the action spec.\"\"\"\n",
        "    if self._action_cb:\n",
        "      return self._action_cb.action_spec()\n",
        "    return specs.BoundedArray((),\n",
        "                              minimum=[],\n",
        "                              maximum=[],\n",
        "                              dtype=np.float64,\n",
        "                              name=\"dummy_action\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC3UUOuqgLpk"
      },
      "source": [
        "## Main Loop\n",
        "\n",
        "We can finally build our graph and run it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b-fM8d1uts7"
      },
      "outputs": [],
      "source": [
        "# Stubs for pulling observation and sending action to some external system.\n",
        "observation_cb = ExampleObservationUpdater()\n",
        "action_cb = ExampleActionSender()\n",
        "\n",
        "# Create an environment that forwards the observation and action calls.\n",
        "env = ProxyEnvironment(observation_cb, action_cb)\n",
        "\n",
        "# Stub policy that runs the desired agent.\n",
        "policy = ExamplePolicy(action_cb.action_spec(), \"agent\")\n",
        "\n",
        "# Wrap policy into an agent that logs to the terminal.\n",
        "task = ExampleSubTask(env.observation_spec(), action_cb.action_spec(), 10)\n",
        "logger = print_logger.PrintLogger()\n",
        "aggregator = subtask_logger.EpisodeReturnAggregator()\n",
        "logging_observer = subtask_logger.SubTaskLogger(logger, aggregator)\n",
        "agent = subtask.SubTaskOption(task, policy, [logging_observer])\n",
        "\n",
        "reset_op = ExampleScriptedOption(action_cb.action_spec(), \"reset\", 3)\n",
        "main_loop = loop_ops.Repeat(5, sequence.Sequence([reset_op, agent]))\n",
        "\n",
        "# Run the episode.\n",
        "timestep = env.reset()\n",
        "while True:\n",
        "  action = main_loop.step(timestep)\n",
        "  timestep = env.step(action)\n",
        "\n",
        "  # Terminate if the environment or main_loop requests it.\n",
        "  if timestep.last() or (main_loop.pterm(timestep) \u003e np.random.rand()):\n",
        "    if not timestep.last():\n",
        "      termination_timestep = timestep._replace(step_type=dm_env.StepType.LAST)\n",
        "      main_loop.step(termination_timestep)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "tutorial.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1yha_OoznAJ75nflTd7GPt5xEgzSOE167",
          "timestamp": 1624882425326
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
