{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bs9PR2FXQl2"
      },
      "source": [
        "# \u003cdiv align=\"left\"\u003e**`moma` tutorial**\u003c/div\u003e\n",
        "# \u003cdiv align=\"left\"\u003e[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepmind/dm_robotics/blob/main/py/moma/moma_tutorial.ipynb)\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCTYqlKaWwkx"
      },
      "source": [
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eCopyright 2021 The dm_robotics Authors.\u003c/small\u003e\u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at \u003ca href=\"http://www.apache.org/licenses/LICENSE-2.0\"\u003ehttp://www.apache.org/licenses/LICENSE-2.0\u003c/a\u003e.\u003c/small\u003e\u003c/small\u003e\u003c/p\u003e\n",
        "\u003e \u003cp\u003e\u003csmall\u003e\u003csmall\u003eUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\u003c/small\u003e\u003c/small\u003e\u003c/p\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fsyNvJcW72D"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2C4HHLsX0Kw"
      },
      "outputs": [],
      "source": [
        "#@title All `dm_robotics` imports required for this tutorial\n",
        "\n",
        "# Agentflow provides useful utilities for MoMa.\n",
        "from dm_robotics import agentflow as af\n",
        "from dm_robotics.agentflow import spec_utils\n",
        "from dm_robotics.agentflow.decorators import overrides\n",
        "from dm_robotics.agentflow.preprocessors import observation_transforms\n",
        "from dm_robotics.agentflow.preprocessors import rewards\n",
        "from dm_robotics.agentflow.preprocessors import timestep_preprocessor\n",
        "from dm_robotics.agentflow.subtasks import subtask_termination\n",
        "\n",
        "# The geometry library helps with managing frames and sampling from\n",
        "# random poses.\n",
        "from dm_robotics.geometry import pose_distribution\n",
        "\n",
        "# The MoMa modules used within this tutorial.\n",
        "from dm_robotics.moma import action_spaces\n",
        "from dm_robotics.moma import base_task\n",
        "from dm_robotics.moma import effector\n",
        "from dm_robotics.moma import entity_initializer\n",
        "from dm_robotics.moma import prop\n",
        "from dm_robotics.moma import robot\n",
        "from dm_robotics.moma import sensor\n",
        "from dm_robotics.moma import subtask_env_builder\n",
        "from dm_robotics.moma.models import types\n",
        "from dm_robotics.moma.models.arenas import empty\n",
        "from dm_robotics.moma.models.end_effectors.robot_hands import robot_hand\n",
        "from dm_robotics.moma.models.end_effectors.robot_hands import robotiq_2f85\n",
        "from dm_robotics.moma.models.robots.robot_arms import robot_arm\n",
        "from dm_robotics.moma.models.robots.robot_arms import sawyer\n",
        "from dm_robotics.moma.models.robots.robot_arms import sawyer_constants\n",
        "from dm_robotics.moma.sensors import prop_pose_sensor\n",
        "from dm_robotics.moma.sensors import robot_arm_sensor\n",
        "from dm_robotics.moma.sensors import robotiq_gripper_sensor\n",
        "from dm_robotics.moma.tasks import run_loop\n",
        "\n",
        "# The transformations library provides useful utils for converting between\n",
        "# frames, from quaternions to euler angles, etc.\n",
        "from dm_robotics.transformations import transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5BG0s7TX6Ar"
      },
      "outputs": [],
      "source": [
        "#@title Other imports and helper functions\n",
        "\n",
        "import enum\n",
        "import os\n",
        "from typing import Dict, Sequence, Tuple, Union\n",
        "\n",
        "from dm_control import composer\n",
        "from dm_control import mjcf\n",
        "from dm_control import mujoco\n",
        "from dm_control.composer.observation import observable\n",
        "from dm_env import specs\n",
        "from IPython.display import HTML\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "# To create and save animations, make sure FFmpeg is installed on your server.\n",
        "# Check a path to FFmpeg binary with `which ffmpeg`, and use the output\n",
        "# in the notebook, for example:\n",
        "# matplotlib.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg`\n",
        "\n",
        "\n",
        "def render_scene(physics: mjcf.Physics) -\u003e np.ndarray:\n",
        "  camera = mujoco.MovableCamera(physics, height=360, width=480)\n",
        "  camera.set_pose([0, 0, 0], 2.5, 180, -35)\n",
        "  return camera.render()\n",
        "\n",
        "def display_images(images, titles):\n",
        "  assert len(images) == len(titles)\n",
        "  fig, axs = plt.subplots(1, len(images))\n",
        "  for ax, img, title in zip(axs, images, titles):\n",
        "    ax.imshow(img)\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    ax.set_title(title)\n",
        "  fig.set_size_inches(12, 12)\n",
        "\n",
        "# Inline video helper function\n",
        "if os.environ.get('COLAB_NOTEBOOK_TEST', False):\n",
        "  # We skip video generation during tests, as it is quite expensive.\n",
        "  display_video = lambda *args, **kwargs: None\n",
        "else:\n",
        "  def display_video(frames, framerate=30):\n",
        "    height, width, _ = frames[0].shape\n",
        "    dpi = 70\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "      im.set_data(frame)\n",
        "      return [im]\n",
        "    interval = 1000/framerate\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                   interval=interval, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKdnInZ_bl3G"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi7c77UacCCB"
      },
      "source": [
        "MoMa (Modular Manipulation) is DeepMind's library for building robotic manipulation tasks for reinforcement learning (RL). In this tutorial, we will run through important MoMa concepts while building a \"lift-a-block\" task with a Sawyer robot arm, a Robotiq parallel gripper, and some objects.\n",
        "\n",
        "For a more complex example, see `tasks/example_task/` in the project folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlqwKfGcUXL"
      },
      "source": [
        "## The `dm_env` Environment interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20D4HbSqex0h"
      },
      "source": [
        "In RL tasks, there is typically an \"agent\" and an \"environment\", where the agent provides actions and the environment responds with observations and rewards. There are many ways to codify that agent-environment interaction, and within this tutorial, we will use the [`dm_env.Environment`](https://github.com/deepmind/dm_env) interface where the agent provides actions through a list of floats and the environment responds with a `dm_env.TimeStep` consisting of observations and rewards.\n",
        "\n",
        "Note that MoMa is not tied to this environment interface. For other, more complex uses of MoMa, see the `dm_robotics` Agentflow module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w00h0JT_6DWv"
      },
      "source": [
        "# Build the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzY2SscB6Uhl"
      },
      "source": [
        "There are 3 main components of every MoMa task: the MuJoCo model of the scene, the hardware abstraction, and the task-specific logic.\n",
        "\n",
        "For this tutorial, we will work with a single Sawyer arm, a Robotiq parallel gripper, and a block in simulation  to build a \"lift-a-block\" task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpPIZuZA9fon"
      },
      "source": [
        "## Build the scene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZN22Qgy9ihc"
      },
      "source": [
        "MoMa is built on top of Composer, which is part of [`dm_control`](https://github.com/deepmind/dm_control), a DeepMind library for managing MuJoCo simulation environments.\n",
        "\n",
        "Before we can start writing our task, we need to build the scene in simulation, which we can do using Composer tools.\n",
        "\n",
        "**NOTE**: Even if you are creating a task for real robots (which MoMa supports as well!), you must do this step. MoMa requires a working MuJoCo model of the scene for both simulation and real tasks. The MuJoCo model is more useful in the simulation case, but even for real-world environments, it is used to help with computing observations and IK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYM3f7-C-ExF"
      },
      "outputs": [],
      "source": [
        "def _build_arena(name: str) -\u003e composer.Arena:\n",
        "  \"\"\"Returns an empty arena where we can put everything.\"\"\"\n",
        "  arena = empty.Arena(name)\n",
        "  arena.mjcf_model.option.timestep = 0.001\n",
        "  arena.mjcf_model.option.gravity = (0., 0., -1.0)\n",
        "  arena.mjcf_model.size.nconmax = 1000\n",
        "  arena.mjcf_model.size.njmax = 2000\n",
        "  arena.mjcf_model.visual.__getattr__('global').offheight = 480\n",
        "  arena.mjcf_model.visual.__getattr__('global').offwidth = 640\n",
        "  arena.mjcf_model.visual.map.znear = 0.0005\n",
        "  return arena\n",
        "\n",
        "def _add_robot_and_gripper(\n",
        "    arena: composer.Arena) -\u003e Tuple[composer.Entity, composer.Entity]:\n",
        "  arm = sawyer.Sawyer(\n",
        "      name='sawyer', actuation=sawyer_constants.Actuation.INTEGRATED_VELOCITY)\n",
        "  gripper = robotiq_2f85.Robotiq2F85()\n",
        "  # Attach the gripper to the arm.\n",
        "  robot.standard_compose(arm=arm, gripper=gripper)\n",
        "  # And attach arm to the arena.\n",
        "  arena.attach(arm)\n",
        "  return arm, gripper\n",
        "\n",
        "def _add_block(arena: composer.Arena) -\u003e composer.Entity:\n",
        "  block = prop.Block()\n",
        "  frame = arena.add_free_entity(block)\n",
        "  block.set_freejoint(frame.freejoint)\n",
        "  return block\n",
        "\n",
        "# Build the scene.\n",
        "arena = _build_arena('sawyer_with_block')\n",
        "arm, gripper = _add_robot_and_gripper(arena)\n",
        "block = _add_block(arena)\n",
        "\n",
        "# Visualize it.\n",
        "physics = mjcf.Physics.from_mjcf_model(arena.mjcf_model)\n",
        "PIL.Image.fromarray(render_scene(physics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nonr081m4tI7"
      },
      "source": [
        "In the visualization above, you can see the Sawyer arm with the Robotiq gripper. The block may not be visible, but we will fix that once we get to the initializers section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_5TnNH86rMP"
      },
      "source": [
        "## Hardware abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohh9RMBQ6v0I"
      },
      "source": [
        "In MoMa, we abstract away the physical components in an environment with 2 classes: `sensor.Sensor` and `effector.Effector`:\n",
        "\n",
        "```python\n",
        "from dm_robotics.moma import effector\n",
        "from dm_robotics.moma import sensor\n",
        "```\n",
        "\n",
        "A `Sensor` produces observations and an `Effector` consumes actions and actuates parts of the environment.\n",
        "\n",
        "Now, let's create some sensors and effectors for the physical setup we have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZbgtPzc5NKZ"
      },
      "source": [
        "### Effectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CNC_nGZ5P1A"
      },
      "source": [
        "Let's control the Sawyer with joint velocity commands, and let's control the parallel gripper with 1D velocity commands.\n",
        "\n",
        "We already have several basic effectors already available in MoMa that provide joint control, Cartesian control, etc. But for this tutorial, we will implement the joint velocity effector from scratch.\n",
        "\n",
        "For existing effectors, see `moma/effectors/`.\n",
        "\n",
        "Every MoMa effector simply needs to extend the abstract base class in `effector.py` in order to (a) give a spec for the expected commands to that effector and (b) define how it consumes commands that adhere to its spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdFVdlK965nL"
      },
      "outputs": [],
      "source": [
        "class JointVelocityEffector(effector.Effector):\n",
        "  \"\"\"Actuate all the velocity MuJoCo actuators in an arm or gripper.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               entity: Union[robot_arm.RobotArm, robot_hand.RobotHand],\n",
        "               name: str):\n",
        "    \"\"\"JointVelocityEffector constructor.\n",
        "\n",
        "    Args:\n",
        "      entity: A robot arm or gripper with an `actuators` property. This class\n",
        "        assumes that the actuators are all in velocity or integrated velocity\n",
        "        mode.\n",
        "      name: Name of the hardware being controlled.\n",
        "    \"\"\"\n",
        "    self._entity = entity\n",
        "    self._name = name\n",
        "    self._action_spec: specs.BoundedArray = None\n",
        "\n",
        "  def after_compile(self, mjcf_model: mjcf.RootElement) -\u003e None:\n",
        "    \"\"\"Called after the MJCF model has been compiled and finalized.\"\"\"\n",
        "    pass  # We don't need to do anything special here.\n",
        "\n",
        "  def initialize_episode(self, physics: mjcf.Physics,\n",
        "                         random_state: np.random.RandomState) -\u003e None:\n",
        "    \"\"\"Called before each episode.\"\"\"\n",
        "    pass  # This effector is not stateful, so no initialization is needed.\n",
        "\n",
        "  def action_spec(self, physics: mjcf.Physics) -\u003e specs.BoundedArray:\n",
        "    \"\"\"Returns details for what kind of command is expected by set_control.\"\"\"\n",
        "    if self._action_spec is None:\n",
        "      num_actuators = len(self._entity.actuators)\n",
        "      actuator_names = [f'{self.prefix}{i}'\n",
        "                        for i in range(num_actuators)]\n",
        "      action_min, action_max = self._action_range_from_actuators(\n",
        "          physics, self._entity.actuators)\n",
        "      self._action_spec = specs.BoundedArray(\n",
        "          shape=(num_actuators,),\n",
        "          dtype=np.float32,\n",
        "          minimum=action_min,\n",
        "          maximum=action_max,\n",
        "          name='\\t'.join(actuator_names))\n",
        "    return self._action_spec\n",
        "\n",
        "  def set_control(self, phys: mjcf.Physics, command: np.ndarray) -\u003e None:\n",
        "    \"\"\"Actuates all the actuators in the entity.\"\"\"\n",
        "    spec_utils.validate(self.action_spec(phys), command)\n",
        "    phys.bind(self._entity.actuators).ctrl = command\n",
        "\n",
        "  @property\n",
        "  def prefix(self) -\u003e str:\n",
        "    return f'{self._name}_actuator'\n",
        "\n",
        "  def _action_range_from_actuators(\n",
        "      self,\n",
        "      physics: mjcf.Physics,\n",
        "      actuators: Sequence[types.MjcfElement]) -\u003e Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Returns the action range min, max for the actuators.\"\"\"\n",
        "    num_actions = len(actuators)\n",
        "    control_range = physics.bind(actuators).ctrlrange\n",
        "    is_limited = physics.bind(actuators).ctrllimited.astype(np.bool)\n",
        "    minima = np.full(num_actions, fill_value=-np.inf, dtype=np.float32)\n",
        "    maxima = np.full(num_actions, fill_value=np.inf, dtype=np.float32)\n",
        "    minima[is_limited], maxima[is_limited] = control_range[is_limited].T\n",
        "    return minima, maxima\n",
        "\n",
        "# Create effectors for the arm and gripper.\n",
        "# (minor note: the actuators in a Robotiq gripper are not technically\n",
        "# joint-level because of coupling in Robotiq 2F85 grippers, but that doesn't\n",
        "# practically affect what we are doing here.)\n",
        "arm_joint_effector = JointVelocityEffector(entity=arm, name='sawyer')\n",
        "gripper_effector = JointVelocityEffector(entity=gripper, name='robotiq')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oIhyb0xB3GN"
      },
      "outputs": [],
      "source": [
        "# Visualize the starting state.\n",
        "before = render_scene(physics)\n",
        "\n",
        "# Try running the effectors in a loop.\n",
        "num_steps = int(1. / physics.timestep())  # run for a second\n",
        "arm_command = np.ones_like(\n",
        "    arm_joint_effector.action_spec(physics).minimum) * 0.5\n",
        "gripper_command = np.ones_like(\n",
        "    gripper_effector.action_spec(physics).minimum) * 0.5\n",
        "for _ in range(num_steps):\n",
        "  arm_joint_effector.set_control(physics, arm_command)\n",
        "  gripper_effector.set_control(physics, gripper_command)\n",
        "  physics.step()\n",
        "\n",
        "# Visualize the new state of things.\n",
        "after = render_scene(physics)\n",
        "\n",
        "display_images([before, after], ['Before actuation', 'After actuation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HXTKMH7Hbbf"
      },
      "source": [
        "### Sensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ddp_MECHc9J"
      },
      "source": [
        "Sensors in MoMa extend the abstract base class defined in `sensor.py`. They provide observations via Composer observables. See `dm_control.composer` for more info on the observable API.\n",
        "\n",
        "We have several existing sensor implementations in `moma/sensors/`. In this tutorial, let's implement a simple sensor that will give us the pose of the gripper in the world-frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lllnvx_8JIqf"
      },
      "outputs": [],
      "source": [
        "@enum.unique\n",
        "class GripperPoseObservations(enum.Enum):\n",
        "  \"\"\"Observations exposed by this sensor.\n",
        "  \n",
        "  Typically, for each MoMa sensor we provide a corresponding enum that tracks\n",
        "  all the available observations for the sensor. It helps with programatically\n",
        "  fetching observations later when we get to the RL environment.\n",
        "  \"\"\"\n",
        "  # The world x,y,z position of the gripper's tool-center-point.\n",
        "  POS = '{}_pos'\n",
        "  # The world orientation quaternion of the gripper's tool-center-point.\n",
        "  QUAT = '{}_quat'\n",
        "\n",
        "  def get_obs_key(self, name: str) -\u003e str:\n",
        "    \"\"\"Returns the key to the observation in the observables dict.\"\"\"\n",
        "    return self.value.format(name)\n",
        "\n",
        "class GripperPoseSensor(sensor.Sensor):\n",
        "\n",
        "  def __init__(self, gripper: robot_hand.RobotHand, name: str):\n",
        "    self._gripper = gripper\n",
        "    self._name = name\n",
        "\n",
        "    # Mapping of observation names to the composer observables (callables that\n",
        "    # will produce numpy arrays containing the observations).\n",
        "    self._observables = {\n",
        "        self.get_obs_key(GripperPoseObservations.POS):\n",
        "            observable.Generic(self._pos),\n",
        "        self.get_obs_key(GripperPoseObservations.QUAT):\n",
        "            observable.Generic(self._quat),\n",
        "    }\n",
        "    for obs in self._observables.values():\n",
        "      obs.enabled = True\n",
        "\n",
        "  def initialize_episode(self, physics: mjcf.Physics,\n",
        "                         random_state: np.random.RandomState) -\u003e None:\n",
        "    pass  # Nothing special needed each episode.\n",
        "\n",
        "  @property\n",
        "  def observables(self) -\u003e Dict[str, observable.Observable]:\n",
        "    return self._observables\n",
        "\n",
        "  @property\n",
        "  def name(self) -\u003e str:\n",
        "    return self._name\n",
        "\n",
        "  def get_obs_key(self, obs: GripperPoseObservations) -\u003e str:\n",
        "    return obs.get_obs_key(self._name)\n",
        "\n",
        "  def _pos(self, physics: mjcf.Physics) -\u003e np.ndarray:\n",
        "    return physics.bind(self._gripper.tool_center_point).xpos\n",
        "\n",
        "  def _quat(self, physics: mjcf.Physics) -\u003e np.ndarray:\n",
        "    rmat = physics.bind(self._gripper.tool_center_point).xmat\n",
        "    quat = transformations.mat_to_quat(np.reshape(rmat, [3, 3]))\n",
        "    return transformations.positive_leading_quat(quat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNQd9BjwLxFU"
      },
      "outputs": [],
      "source": [
        "# Create a sensor for the gripper.\n",
        "gripper_pose_sensor = GripperPoseSensor(gripper, 'gripper')\n",
        "\n",
        "# Check out the poses returned by the sensor.\n",
        "for observation_name, obs_callable in gripper_pose_sensor.observables.items():\n",
        "  print(f'{observation_name}: {obs_callable(physics)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB2xm14bh_1l"
      },
      "source": [
        "### Switching between simulated and real robots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOxzTW0iYM4"
      },
      "source": [
        "The `Effector` and `Sensor` classes are usable by both simulated entities _and_ real-world entities. For instance, the velocity effector we wrote above can easily be replaced by an `Effector` that publishes joint velocity commands via ROS to a physical robot.\n",
        "\n",
        "The rest of this tutorial will cover the rest of the task logic we can configure within MoMa, but it is important to note that, in order to move from a simulated task to a real-world one, all you need to do is change the sensors and effectors to their real-world equivalents. The rest of MoMa is agnostic to sim vs. real.\n",
        "\n",
        "As noted above, even for real robot environments, you still need to provide a MuJoCo model of the scene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhynbHa8oY-s"
      },
      "source": [
        "## MoMa `BaseTask`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GUHnHr2ody1"
      },
      "source": [
        "The MuJoCo scene, sensors, and effectors represent the basis of any task we want to build with this physical setup. We could create a task where the arm needs to reach the block or where it needs to pick-and-place the block to a new location. We could change the types of observations we want to actually surface to the agent or add new observations (like the distance from the gripper to the block).\n",
        "\n",
        "All of those extra bits are task-specific, but the physical basis remains constant. In MoMa, we wrap that basis with a class `BaseTask` (see `base_task.py`).\n",
        "\n",
        "`BaseTask` is the simplest possible task we could give to an agent. It provides a zero reward, an empty action spec (the agent can't do anything), and all the possible observations from all the sensors registered. In later sections, we will discuss how to build on top of `BaseTask` to make more interesting tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rskB_l-ZoYDt"
      },
      "outputs": [],
      "source": [
        "# Rather than deal with the raw Compose entities, we wrap the arms and grippers,\n",
        "# along with their relevant effectors and sensors, with a `Robot` abstraction.\n",
        "robot_sensors = [\n",
        "    # Sensor for the joint angles, velocities, and torques.\n",
        "    robot_arm_sensor.RobotArmSensor(\n",
        "        arm=arm, name='sawyer', have_torque_sensors=True),\n",
        "    # Sensor for the gripper position in the world frame (see prev section.)\n",
        "    gripper_pose_sensor,\n",
        "    # Sensor for the gripper state (width of the fingers).\n",
        "    robotiq_gripper_sensor.RobotiqGripperSensor(gripper, name='gripper_state'),\n",
        "]\n",
        "sawyer_and_gripper = robot.StandardRobot(\n",
        "    arm=arm,\n",
        "    arm_base_site_name='pedestal_attachment',\n",
        "    gripper=gripper,\n",
        "    robot_sensors=robot_sensors,\n",
        "    arm_effector=arm_joint_effector,\n",
        "    gripper_effector=gripper_effector)\n",
        "\n",
        "# Keep track of the pose of the block, in the world frame.\n",
        "block_pose_sensor = prop_pose_sensor.PropPoseSensor(block, name='block')\n",
        "\n",
        "task = base_task.BaseTask(\n",
        "    task_name='sawyer_lift_block',\n",
        "    arena=arena,\n",
        "    robots=[sawyer_and_gripper],\n",
        "    props=[block],\n",
        "    extra_sensors=[block_pose_sensor],\n",
        "    extra_effectors=[],  # Add any effectors not associated with a robot here.\n",
        "    control_timestep=0.1,  # Run the task at 10 Hz.\n",
        "    # We will revisit the initializes in a later section.\n",
        "    scene_initializer=lambda _: None,\n",
        "    episode_initializer=lambda _1, _2: None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Ifof5-jigh"
      },
      "source": [
        "## Task logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkXxy6rHjm_8"
      },
      "source": [
        "Through the scene and hardware abstraction, we've built the foundation to design any task we want with a Sawyer, a gripper, and a single block. Now, we need to answer 3 questions:\n",
        "\n",
        "1. What action space do we want to expose to our agents?\n",
        "2. How do we want to initialize our episodes?\n",
        "3. What rewards, episode termination criteria, etc. do we want?\n",
        "\n",
        "This seciton runs through those 3 questions for our task: lifting the block with the arm and gripper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajOmJ0FTkh97"
      },
      "source": [
        "### Action spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3_SZ5ngkk9l"
      },
      "source": [
        "MoMa borrows the concept of an action space from DeepMind's Agentflow. An `af.ActionSpace` projects an action (a Numpy array) to a new space (another Numpy array).\n",
        "\n",
        "You can find commonly used `af.ActionSpace` subclasses within `action_spaces.py`. Important note: in practice, we actually do a lot of the projecting from one action space to another inside the `Effector` classes. For instance, the `cartesian_6d_velocity_effector.py` projects 6D Cartesian velocity commands to joint space, and then delegates those commands to an underlying effector it wraps. However, we could do that via an `af.ActionSpace` instead.\n",
        "\n",
        "For this tutorial, let's not worry about projecting action spaces, but rather, let's combine action spaces via the `af.CompositeActionSpace`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAxPIqmzm07h"
      },
      "outputs": [],
      "source": [
        "# First get the complete action spec (not space!) from the base task.\n",
        "parent_action_spec = task.effectors_action_spec(physics)\n",
        "\n",
        "# Then create an action space associated with each effector.\n",
        "# Joint space control for the arm.\n",
        "joint_action_space = action_spaces.ArmJointActionSpace(\n",
        "    af.prefix_slicer(parent_action_spec, arm_joint_effector.prefix))\n",
        "# 1D control for the gripper.\n",
        "gripper_action_space = action_spaces.GripperActionSpace(\n",
        "    af.prefix_slicer(parent_action_spec, gripper_effector.prefix))\n",
        "\n",
        "# If we didn't want to give raw control to all the joints of the arm, or if we\n",
        "# wanted to remove control of the gripper, we could add such logic here and\n",
        "# modify the action space. However, for this task, we will keep it simple and\n",
        "# just use each action space as is.\n",
        "\n",
        "# Combine them into a composite action space, which we can expose to the agents.\n",
        "combined_action_space = af.CompositeActionSpace(\n",
        "    [joint_action_space, gripper_action_space])\n",
        "\n",
        "# Check the spec of the action space, and you should see 8 actions: 7 for the\n",
        "# arm's joints and 1 for the gripper.\n",
        "combined_action_space.spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vie7r7EmCPF"
      },
      "source": [
        "Note: Even though the action space defined above is fairly trivial, it is necessary for building the RL environment. Remember that the `BaseTask` we defined above has a **empty** action spec, meaning the agent cannot give any actions. We will use `combined_action_space` later to allow the RL agent to provide non-empty actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85NQO3SWmrum"
      },
      "source": [
        "### Initializing episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iS7oGFmuwO"
      },
      "source": [
        "In RL, the agent-environment interactions are often split up into \"episodes\" with a start and end. Before the start of each episode, we often will want to reset the environment and put entities in randomized locations. In MoMa, we have a set of scene and episode initializers to help do that.\n",
        "\n",
        "A `SceneInitializer` is a callable that changes the MJCF (MuJoCo model) of the scene. Usually this entails moving entities around and changing their attachment sites or fixed joint positions, but it could also involve changing friction, lighting, etc. for domain randomization. It is typically not used for real-world environments.\n",
        "\n",
        "An episode initializer is a `composer.Initializer` that is run after the `SceneInitializer` is run and the MJCF is recompiled. You can find a few useful initializers within `entity_initializer.py`.\n",
        "\n",
        "Scene initialization is outside the scope of this tutorial. Instead, we will focus on initializing 2 things: the joints of the arm and the position of the block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exLfr6_1qUSv"
      },
      "outputs": [],
      "source": [
        "# To initialize the arm joints in a useful starting position, let's:\n",
        "#   1. Sample a random goal pose for the gripper within some 3D bounding box.\n",
        "#   2. Do IK to calculate the arm joint angles needed to place the gripper at\n",
        "#      that goal pose.\n",
        "# Luckily, there are out-of-the-box MoMa utilities we can use to do this.\n",
        "gripper_pose_dist = pose_distribution.UniformPoseDistribution(\n",
        "    # Provide 6D min and max bounds for the 3D position and 3D euler angles.\n",
        "    min_pose_bounds=np.array([0.5, -0.1, 0.1,\n",
        "                              0.75 * np.pi, -0.25 * np.pi, -0.5 * np.pi]),\n",
        "    max_pose_bounds=np.array([0.7, 0.1, 0.2,\n",
        "                              1.25 * np.pi, 0.25 * np.pi, 0.5 * np.pi]))\n",
        "initialize_arm = entity_initializer.PoseInitializer(\n",
        "    # `position_gripper()` is a function that performs the IK and moves the\n",
        "    # joints of the arm to the input gripper pose.\n",
        "    sawyer_and_gripper.position_gripper, gripper_pose_dist.sample_pose)\n",
        "\n",
        "# To initialize the block, we can run through a similar process of sampling a\n",
        "# pose and then moving the block to that pose.\n",
        "block_pose_dist = gripper_pose_dist = pose_distribution.UniformPoseDistribution(\n",
        "    # Provide 6D min and max bounds for the 3D position and 3D euler angles.\n",
        "    min_pose_bounds=np.array([0.5, -0.1, 0.05, 0.0, 0.0, -np.pi]),\n",
        "    max_pose_bounds=np.array([0.7, 0.1, 0.05, 0.0, 0.0, np.pi]))\n",
        "initialize_block = entity_initializer.PoseInitializer(\n",
        "    block.set_pose, pose_sampler=block_pose_dist.sample_pose)\n",
        "\n",
        "# Combine the 2 initializers so they both run.\n",
        "entities_initializer = entity_initializer.TaskEntitiesInitializer(\n",
        "    [initialize_arm, initialize_block])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdSmX14CyiVz"
      },
      "outputs": [],
      "source": [
        "# Run the initializer and see what the scene looks like.\n",
        "before = render_scene(physics)\n",
        "entities_initializer(physics, np.random.RandomState())\n",
        "physics.step()  # propogate the changes from the initializer.\n",
        "after = render_scene(physics)\n",
        "\n",
        "# Visualize the new state of things.\n",
        "display_images([before, after], ['Before init', 'After init'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJtatdXP0awL"
      },
      "source": [
        "Notice that you can see the block now! The gripper should be pointing somewhat downwards above the block (not directly above necessarily). Feel free to change the random state and see more poses the entities can end up in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPya8N3p00QE"
      },
      "source": [
        "### Timestep preprocessors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e1lz05a04fF"
      },
      "source": [
        "Timestep preprocessors are a powerful tool MoMa borrows from DeepMind's Agentflow. An environment timestep is a collection of the rewards and observations coming from the environment. The timestep preprocessors modify the timesteps coming from the `BaseTask` before giving them to the agent.\n",
        "\n",
        "Within the timestep preprocessors, we can do many things including:\n",
        " - Add/remove/modify observations\n",
        " - Add rewards\n",
        " - Terminate an episode\n",
        "\n",
        "These define the task-specific logic of an RL environment. For instance, to change an environment from a lift-a-block task to a pick-and-place task, all you need to do is change the reward function in the timestep preprocessors.\n",
        "\n",
        "You can find many useful timestep preprocessors in `dm_robotics.agentflow.preprocessors`. \n",
        "\n",
        "For our task, let's some existing timestep preprocessors and define a new one as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVRq4LpYK1Z2"
      },
      "outputs": [],
      "source": [
        "preprocessors = []\n",
        "\n",
        "# Make sure everything has the same dtype (simplifies some logic down the line).\n",
        "preprocessors.append(observation_transforms.CastPreprocessor(dtype=np.float32))\n",
        "\n",
        "# Let's add a new observation that gives a vector from the gripper position to\n",
        "# the block position, which could help the agent learn to reach the block.\n",
        "# Note that this is possible via observation_transforms.AddObservation as well,\n",
        "# but we'll implement it here just to show the TimestepPreprocessor API.\n",
        "class AddGripperToBlockObservation(timestep_preprocessor.TimestepPreprocessor):\n",
        "\n",
        "  # Name of the observation this adds.\n",
        "  OBS_KEY: str = 'gripper_to_block'\n",
        "\n",
        "  def __init__(self, gripper_obs: str, block_obs: str):\n",
        "    self._gripper_obs = gripper_obs\n",
        "    self._block_obs = block_obs\n",
        "    super().__init__()\n",
        "\n",
        "  @overrides(timestep_preprocessor.TimestepPreprocessor)\n",
        "  def _process_impl(\n",
        "      self, timestep: timestep_preprocessor.PreprocessorTimestep\n",
        "      ) -\u003e timestep_preprocessor.PreprocessorTimestep:\n",
        "    gripper = timestep.observation[self._gripper_obs]\n",
        "    block = timestep.observation[self._block_obs]\n",
        "    timestep.observation[AddGripperToBlockObservation.OBS_KEY] = (\n",
        "        gripper[:3] - block[:3])\n",
        "    return timestep\n",
        "\n",
        "  @overrides(timestep_preprocessor.TimestepPreprocessor)\n",
        "  def _output_spec(\n",
        "      self, input_spec: spec_utils.TimeStepSpec) -\u003e spec_utils.TimeStepSpec:\n",
        "    obs_spec = dict(input_spec.observation_spec)\n",
        "    obs_spec[AddGripperToBlockObservation.OBS_KEY] = (\n",
        "        specs.Array(shape=(3,), dtype=np.float32))\n",
        "    return input_spec.replace(observation_spec=obs_spec)\n",
        "\n",
        "preprocessors.append(\n",
        "    AddGripperToBlockObservation(\n",
        "        gripper_pose_sensor.get_obs_key(GripperPoseObservations.POS),\n",
        "        block_pose_sensor.get_obs_key(prop_pose_sensor.Observations.POSE)))\n",
        "\n",
        "\n",
        "# Let's only expose a subset of the observations coming from the sensors to the\n",
        "# agent.\n",
        "preprocessors.append(\n",
        "    observation_transforms.RetainObservations([\n",
        "        'sawyer_joint_pos',\n",
        "        'sawyer_joint_vel',\n",
        "        'gripper_pos',\n",
        "        'gripper_quat',\n",
        "        'block_pose',\n",
        "        'gripper_to_block',\n",
        "    ], raise_on_missing=True))\n",
        "\n",
        "\n",
        "# Let's add a reward for lifting the block up. We will have a 2-part reward -\n",
        "# some of the reward comes from getting close to the block and some of the\n",
        "# reward comes from actually lifting up the block. This shaping should help\n",
        "# with learning a lift policy.\n",
        "def distance_from_block_reward(\n",
        "    observations: spec_utils.ObservationValue) -\u003e float:\n",
        "  gripper_to_block_distance = np.linalg.norm(\n",
        "      observations[AddGripperToBlockObservation.OBS_KEY])\n",
        "  return np.clip(1.0 - gripper_to_block_distance, 0, 1)\n",
        "\n",
        "def lift_reward(observations: spec_utils.ObservationValue) -\u003e float:\n",
        "  obs_key = block_pose_sensor.get_obs_key(prop_pose_sensor.Observations.POSE)\n",
        "  block_height = observations[obs_key][2]\n",
        "  return np.clip(block_height * 2., 0, 1)\n",
        "\n",
        "def sum_rewards(rewards: Sequence[float]) -\u003e float:\n",
        "  return np.clip(np.sum(rewards), 0, 1)\n",
        "\n",
        "preprocessors.append(\n",
        "    rewards.CombineRewards(\n",
        "        reward_preprocessors=[\n",
        "            rewards.ComputeReward(distance_from_block_reward),\n",
        "            rewards.ComputeReward(lift_reward),\n",
        "        ], combination_strategy=sum_rewards))\n",
        "\n",
        "\n",
        "# Terminate the episodes after 5 sec. We are running at 10 Hz, so that means\n",
        "# 50 environment steps.\n",
        "preprocessors.append(subtask_termination.MaxStepsTermination(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtODY8wOyElr"
      },
      "source": [
        "## Make the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWGlc_5eyGcZ"
      },
      "source": [
        "Great! Now we have all the components needed for an RL task:\n",
        " - a MuJoCo scene\n",
        " - the sensors and effectors to interact with the physical environment\n",
        " - task-specific logic in the form of action spaces, initializers, and timestep preprocessors\n",
        "\n",
        "We can combine these into an RL environment by using MoMa's `SubtaskEnvironment`. The name \"subtask\" comes from Agentflow. Agentflow allows an agent to _flow_ from one task to the next, so each of the agent's objectives are really sub-tasks while the agent is completing a larger overarching goal.\n",
        "\n",
        "However, in our case, we just have a single subtask that we would like to encapsulate as an RL environment. More specifically, we would like to adhere to DeepMind's canonical RL environment interface: `dm_env.Environment`. The MoMa `SubtaskEnvironment` helps do exactly that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfMeJ5g9z5Qb"
      },
      "outputs": [],
      "source": [
        "# Add the initializer to the base task.\n",
        "task.set_episode_initializer(entities_initializer)\n",
        "\n",
        "env_builder = subtask_env_builder.SubtaskEnvBuilder()\n",
        "env_builder.set_task(task)\n",
        "env_builder.set_action_space(combined_action_space)\n",
        "for preprocessor in preprocessors:\n",
        "  env_builder.add_preprocessor(preprocessor)\n",
        "environment = env_builder.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMguPWBE2C05"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the action, observation and reward specs.\n",
        "print(f'Action spec: {environment.action_spec()}')\n",
        "print('Observation spec:')\n",
        "obs_spec = environment.observation_spec()\n",
        "for key, spec in obs_spec.items():\n",
        "  print(f'\\t{key}: {spec}')\n",
        "print(f'Reward spec: {environment.reward_spec()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WJw9FFz6F47"
      },
      "source": [
        "# Run the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIuruXFM58bp"
      },
      "source": [
        "MoMa provides a `run_loop` to help step the agent and environment. It also provides useful tools for observing  the timesteps while pass to the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2OAbNCx4mb6"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "  \"\"\"An agent that emits uniform random actions.\"\"\"\n",
        "\n",
        "  def __init__(self, action_spec: specs.BoundedArray):\n",
        "    self._spec = action_spec\n",
        "    self._shape = action_spec.shape\n",
        "    self._range = action_spec.maximum - action_spec.minimum\n",
        "\n",
        "  def step(self, unused_timestep):\n",
        "    action = (np.random.rand(*self._shape) - 0.5) * self._range\n",
        "    np.clip(action, self._spec.minimum, self._spec.maximum, action)\n",
        "    return action.astype(self._spec.dtype)\n",
        "\n",
        "class CameraObserver:\n",
        "\n",
        "  def __init__(self, physics_getter):\n",
        "    self._physics_getter = physics_getter\n",
        "    self._cam = None\n",
        "    self._frames = []\n",
        "\n",
        "  def begin_episode(self, *unused_args, **unused_kwargs):\n",
        "    self._frames.append(render_scene(self._physics_getter()))\n",
        "\n",
        "  def step(self, *unused_args, **unused_kwargs):\n",
        "    self._frames.append(render_scene(self._physics_getter()))\n",
        "\n",
        "  def end_episode(self, *unused_args, **unused_kwargs):\n",
        "    # Add some black frames in between episodes.\n",
        "    for _ in range(5):\n",
        "      self._frames.append(np.zeros_like(self._frames[-1]))\n",
        "\n",
        "  def get_frames(self):\n",
        "    return self._frames\n",
        "\n",
        "  def reset(self):\n",
        "    self._frames = []\n",
        "\n",
        "\n",
        "agent = RandomAgent(environment.action_spec())\n",
        "observer = CameraObserver(lambda: environment.physics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk-TN-7W8a_7"
      },
      "outputs": [],
      "source": [
        "# Run a couple episodes.\n",
        "observer.reset()\n",
        "run_loop.run(environment, agent, observers=[observer], max_steps=100)\n",
        "\n",
        "# Render the video.\n",
        "display_video(observer.get_frames(),\n",
        "              framerate=int(1./environment.task.control_timestep))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iKdnInZ_bl3G",
        "dLlqwKfGcUXL",
        "9HXTKMH7Hbbf",
        "QB2xm14bh_1l",
        "qhynbHa8oY-s",
        "v7Ifof5-jigh",
        "ajOmJ0FTkh97",
        "85NQO3SWmrum",
        "YPya8N3p00QE",
        "qtODY8wOyElr",
        "8WJw9FFz6F47"
      ],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "moma_tutorial.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1yha_OoznAJ75nflTd7GPt5xEgzSOE167",
          "timestamp": 1624882425326
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
